{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e065b716",
   "metadata": {},
   "source": [
    "#### Objective : Univariate Time Series Modelling using DARTS for model training and weight & Biases for model logging to beat the becnhmark model already provided\n",
    "\n",
    "#### Created By : Ashwini Kumar\n",
    "\n",
    "#### Start Date : 22th March 2022\n",
    "\n",
    "#### Steps Involved :\n",
    "\n",
    "    1. Read the data from the csv file and split into train and validation splits \n",
    "    \n",
    "    2. Try out all the models in DARTs for \"req_ThunB2B_Sorter\" series\n",
    "    \n",
    "    3. Merge the sales order data provided with the original Time Series Data\n",
    "    \n",
    "    3. Find out the model which gives the best performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5ec367bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required packages \n",
    "from darts import TimeSeries\n",
    "import pandas as pd\n",
    "import tqdm as notebook_tqdm\n",
    "from darts.dataprocessing.transformers import Scaler\n",
    "from darts.models import RNNModel, ExponentialSmoothing, BlockRNNModel\n",
    "from darts.metrics import mape\n",
    "from darts.utils.statistics import check_seasonality, plot_acf\n",
    "from darts.datasets import AirPassengersDataset, SunspotsDataset\n",
    "from darts.utils.timeseries_generation import datetime_attribute_timeseries\n",
    "import wandb ## Function for weights and biases\n",
    "import plotly\n",
    "from darts import TimeSeries\n",
    "from darts.dataprocessing.transformers import Scaler\n",
    "from darts.models import RNNModel, ExponentialSmoothing, BlockRNNModel,NBEATSModel,TransformerModel,AutoARIMA,TFTModel\n",
    "from darts.metrics import mape\n",
    "from darts.utils.statistics import check_seasonality, plot_acf\n",
    "from darts.datasets import AirPassengersDataset, SunspotsDataset\n",
    "from darts.utils.timeseries_generation import datetime_attribute_timeseries\n",
    "from darts.models import TCNModel, RNNModel\n",
    "from darts.models import FFT, AutoARIMA, ExponentialSmoothing, Prophet, Theta\n",
    "import os\n",
    "# Remove Noise using the Gaussian Wave\n",
    "from darts.models.forecasting.random_forest import RandomForest\n",
    "\n",
    "from sklearn.gaussian_process.kernels import ExpSineSquared, RBF\n",
    "from darts.models import GaussianProcessFilter\n",
    "# Initialize the product details\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from darts.models.forecasting.regression_model import RegressionModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "092ddec9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>RequestedQTY</th>\n",
       "      <th>GoodsIssuedQTY</th>\n",
       "      <th>InvoicedQTY</th>\n",
       "      <th>PDO number</th>\n",
       "      <th>req_Illy</th>\n",
       "      <th>req_Teddy</th>\n",
       "      <th>req_Thun</th>\n",
       "      <th>req_other</th>\n",
       "      <th>inv_Illy</th>\n",
       "      <th>...</th>\n",
       "      <th>BOL_B2B Sorter BVB Thun</th>\n",
       "      <th>BOL_B2B Sorter riassortimenti WHS</th>\n",
       "      <th>BOL_B2C Thun</th>\n",
       "      <th>BOL_B2C Sorter Thun</th>\n",
       "      <th>BOL_B2C P2L Thun</th>\n",
       "      <th>BOL_Inbound Thun</th>\n",
       "      <th>SO_QTY_20210315</th>\n",
       "      <th>SO_QTY_20210607</th>\n",
       "      <th>SO_QTY_20210830</th>\n",
       "      <th>SO_QTY_20211122</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-01-02</td>\n",
       "      <td>227928.0</td>\n",
       "      <td>217034.0</td>\n",
       "      <td>216049.0</td>\n",
       "      <td>12075</td>\n",
       "      <td>12851</td>\n",
       "      <td>0</td>\n",
       "      <td>215041.0</td>\n",
       "      <td>36</td>\n",
       "      <td>11866</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>168218.0</td>\n",
       "      <td>168218.0</td>\n",
       "      <td>168218.0</td>\n",
       "      <td>168218.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-01-09</td>\n",
       "      <td>176524.0</td>\n",
       "      <td>167839.0</td>\n",
       "      <td>166669.0</td>\n",
       "      <td>10663</td>\n",
       "      <td>14442</td>\n",
       "      <td>0</td>\n",
       "      <td>162078.0</td>\n",
       "      <td>4</td>\n",
       "      <td>13272</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>136751.0</td>\n",
       "      <td>136751.0</td>\n",
       "      <td>136751.0</td>\n",
       "      <td>136751.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-01-16</td>\n",
       "      <td>289548.0</td>\n",
       "      <td>280089.0</td>\n",
       "      <td>279048.0</td>\n",
       "      <td>12330</td>\n",
       "      <td>14555</td>\n",
       "      <td>0</td>\n",
       "      <td>274961.0</td>\n",
       "      <td>32</td>\n",
       "      <td>13514</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>93698.0</td>\n",
       "      <td>93698.0</td>\n",
       "      <td>93698.0</td>\n",
       "      <td>93698.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-01-23</td>\n",
       "      <td>248337.0</td>\n",
       "      <td>234044.0</td>\n",
       "      <td>232787.0</td>\n",
       "      <td>15942</td>\n",
       "      <td>15244</td>\n",
       "      <td>0</td>\n",
       "      <td>233063.0</td>\n",
       "      <td>30</td>\n",
       "      <td>13987</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>77969.0</td>\n",
       "      <td>77969.0</td>\n",
       "      <td>77969.0</td>\n",
       "      <td>77969.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-01-30</td>\n",
       "      <td>190462.0</td>\n",
       "      <td>175915.0</td>\n",
       "      <td>174835.0</td>\n",
       "      <td>9324</td>\n",
       "      <td>13200</td>\n",
       "      <td>0</td>\n",
       "      <td>177195.0</td>\n",
       "      <td>67</td>\n",
       "      <td>12120</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>79360.0</td>\n",
       "      <td>79360.0</td>\n",
       "      <td>79360.0</td>\n",
       "      <td>79360.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date  RequestedQTY  GoodsIssuedQTY  InvoicedQTY  PDO number  req_Illy  \\\n",
       "0 2017-01-02      227928.0        217034.0     216049.0       12075     12851   \n",
       "1 2017-01-09      176524.0        167839.0     166669.0       10663     14442   \n",
       "2 2017-01-16      289548.0        280089.0     279048.0       12330     14555   \n",
       "3 2017-01-23      248337.0        234044.0     232787.0       15942     15244   \n",
       "4 2017-01-30      190462.0        175915.0     174835.0        9324     13200   \n",
       "\n",
       "   req_Teddy  req_Thun  req_other  inv_Illy  ...  BOL_B2B Sorter BVB Thun  \\\n",
       "0          0  215041.0         36     11866  ...                      NaN   \n",
       "1          0  162078.0          4     13272  ...                      NaN   \n",
       "2          0  274961.0         32     13514  ...                      NaN   \n",
       "3          0  233063.0         30     13987  ...                      NaN   \n",
       "4          0  177195.0         67     12120  ...                      NaN   \n",
       "\n",
       "   BOL_B2B Sorter riassortimenti WHS  BOL_B2C Thun  BOL_B2C Sorter Thun  \\\n",
       "0                                NaN           NaN                  NaN   \n",
       "1                                NaN           NaN                  NaN   \n",
       "2                                NaN           NaN                  NaN   \n",
       "3                                NaN           NaN                  NaN   \n",
       "4                                NaN           NaN                  NaN   \n",
       "\n",
       "   BOL_B2C P2L Thun  BOL_Inbound Thun  SO_QTY_20210315  SO_QTY_20210607  \\\n",
       "0               NaN               NaN         168218.0         168218.0   \n",
       "1               NaN               NaN         136751.0         136751.0   \n",
       "2               NaN               NaN          93698.0          93698.0   \n",
       "3               NaN               NaN          77969.0          77969.0   \n",
       "4               NaN               NaN          79360.0          79360.0   \n",
       "\n",
       "   SO_QTY_20210830  SO_QTY_20211122  \n",
       "0         168218.0         168218.0  \n",
       "1         136751.0         136751.0  \n",
       "2          93698.0          93698.0  \n",
       "3          77969.0          77969.0  \n",
       "4          79360.0          79360.0  \n",
       "\n",
       "[5 rows x 65 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to read the csv file from given location & parse datetime properly\n",
    "def load_dataframe(filename):\n",
    "    df = pd.read_csv(filename)\n",
    "    df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "    return df\n",
    "\n",
    "# Function create the Time Series from Dataframe using a Time Series to df function\n",
    "def df_to_timeseries(df,time_column, timeseries):\n",
    "    series = TimeSeries.from_dataframe(df, time_column,timeseries)\n",
    "    return (series)\n",
    "\n",
    "# Define the Mean Absolute Error for evaluating our model\n",
    "from darts.metrics import mae\n",
    "def eval_error(actual_values,pred_values):\n",
    "    return mae(actual_values,pred_values)\n",
    "\n",
    "# Call the function to read the data properly\n",
    "\n",
    "df = load_dataframe(\"C:\\\\Users\\\\ashwini.kumar\\\\Time_Series_Modelling\\\\_Final_master_df_2022_2017_v9_with_Bolzano-sales_joined.csv\")\n",
    "df.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79ce59f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Sum_All_Variables'] = df[\"req_ThunB2B_Sorter\"] + df [\"req_ThunB2C\"] + df[\"req_ThunB2B_AA\"] + df[\"req_ThunB2B_PP\"]\n",
    "df['Sum_All_Variables_Benchmark'] = df[\"BOL_B2B Sorter Thun\"] + df [\"BOL_B2C Thun\"] + df[\"BOL_B2B AA Thun\"] + df[\"BOL_B2B P&P Thun\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10fb35f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function to define the train_data input data and other functions\n",
    "def call_timeseries(train_column,benchmark_column,regression_column,start_period,end_period):\n",
    "    timeseries_input = df_to_timeseries(df,\"Date\",train_column)\n",
    "#     print (timeseries_input)\n",
    "\n",
    "    benchmark_input = df_to_timeseries(df,\"Date\",benchmark_column)\n",
    "    # Set aside the data since 2021 \n",
    "    \n",
    "    regression_input = df_to_timeseries(df,\"Date\",regression_column)\n",
    "    \n",
    "    kernel = ExpSineSquared()\n",
    "    # kernel = RBF()\n",
    "\n",
    "    gpf = GaussianProcessFilter( kernel=kernel, alpha=0.4 / 2, n_restarts_optimizer=100)\n",
    "    \n",
    "    filtered_x = gpf.filter(df_to_timeseries(df,\"Date\",train_column))\n",
    "                            \n",
    "    plt.figure(figsize=[12, 8])\n",
    "    timeseries_input.plot(color=\"red\", label=\"Noisy Time Series\")\n",
    "    filtered_x.plot(color=\"blue\", label=\"Filtered Time Series\")\n",
    "    plt.legend()\n",
    "#     plt.savefig(series_name + 'filtered' + \"/\" + train_column + '.png')\n",
    "    # Split the filtered data into train & validation train_data is useful\n",
    "    train_data , val_data_split = filtered_x.split_before(pd.Timestamp(start_period))\n",
    "    \n",
    "    # Split the Noisy data into train_data noisy & val data noisy. Val data noisy to be used for validation\n",
    "    train_data_noisy, val_data_noisy = timeseries_input.split_before(pd.Timestamp(start_period))\n",
    "    \n",
    "    # Split the benchmark column in train nechmark & bechmark data split\n",
    "    train_benchmark_ignore, benchmark_data_split = benchmark_input.split_before(pd.Timestamp(start_period))\n",
    "    \n",
    "    # Valid data column\n",
    "    val_data, ignore1 = val_data_noisy.split_before(pd.Timestamp(end_period))\n",
    "    \n",
    "    # benchmark data\n",
    "    benchmark_data, ignore1 =  benchmark_data_split.split_before(pd.Timestamp(end_period))\n",
    "    \n",
    "    # Regression data\n",
    "    regression_train_data , regression_val_data_split = regression_input.split_before(pd.Timestamp(end_period))\n",
    "    \n",
    "    # Regression Validation Data\n",
    "#     regression_val_data, ignore2 = regression_val_data_split.split_before(pd.Timestamp(end_period))\n",
    "    \n",
    "    return train_data,val_data,benchmark_data,timeseries_input,regression_train_data, regression_val_data_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39525563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'SO_QTY_20210315','2021-03-22','2021-06-14'\n",
    "# Benchmark Loss is : 36163.0\n",
    "# Model Predict Loss is : 27679.10\n",
    "    \n",
    "# 'SO_QTY_20210607','2021-06-14','2021-09-07'    \n",
    "# Benchmark Loss is : 23325.1\n",
    "# Model Predict Loss is : 21457.3\n",
    "\n",
    "# 'SO_QTY_20210830','2021-09-07','2021-11-29'\n",
    "# Benchmark Loss is : 52058.666\n",
    "# Model Predict Loss is : 125768.23\n",
    "    \n",
    "# \"BOL_B2B Sorter Thun\",'SO_QTY_20211122','2021-11-29','2022-02-07'\n",
    "# Benchmark Loss is : 29709.5\n",
    "# Model Predict Loss is : 28773.8\n",
    "    \n",
    "period_list = [['SO_QTY_20210315','2021-03-22','2021-06-14'],['SO_QTY_20210607','2021-06-14','2021-09-07'],\n",
    "              ['SO_QTY_20210830','2021-09-07','2021-11-29'],['SO_QTY_20211122','2021-11-29','2022-02-07']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fee729c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running of details : ['SO_QTY_20210315', '2021-03-22', '2021-06-14']\n",
      "SO_QTY_20210315 2021-03-22 2021-06-14\n",
      "Running of details : ['SO_QTY_20210607', '2021-06-14', '2021-09-07']\n",
      "SO_QTY_20210607 2021-06-14 2021-09-07\n",
      "Running of details : ['SO_QTY_20210830', '2021-09-07', '2021-11-29']\n",
      "SO_QTY_20210830 2021-09-07 2021-11-29\n",
      "Running of details : ['SO_QTY_20211122', '2021-11-29', '2022-02-07']\n",
      "SO_QTY_20211122 2021-11-29 2022-02-07\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    \n",
    "    periods = period_list[i]\n",
    "    \n",
    "    print (\"Running of details :\",periods)\n",
    "    print (periods[0],periods[1],periods[2])\n",
    "    \n",
    "df_results = pd.DataFrame()\n",
    "\n",
    "train_series = ['Sum_All_Variables'  ]\n",
    "benchmark_series = ['Sum_All_Variables_Benchmark' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8b7d9e64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## define all the models with past covariates\n",
    "\n",
    "\n",
    "model_list_covar = [\n",
    "RegressionModel(lags=24,lags_past_covariates=[-6,-5, -4, -3, -2, -1]),\n",
    "RandomForest(lags=24,lags_past_covariates=[-6,-5, -4, -3, -2, -1]),\n",
    "BlockRNNModel(input_chunk_length=32, output_chunk_length=8, n_rnn_layers=2),\n",
    "TCNModel(input_chunk_length=32, output_chunk_length=8,n_epochs = 120, random_state = 0),\n",
    "NBEATSModel(input_chunk_length=32, output_chunk_length=8,n_epochs = 120, random_state = 0),\n",
    "TransformerModel(input_chunk_length=32, output_chunk_length=8,n_epochs = 120, random_state = 0)]\n",
    "\n",
    "model_list_orig = [\n",
    "RegressionModel(lags=24),\n",
    "RandomForest(lags=24),\n",
    "BlockRNNModel(input_chunk_length=32, output_chunk_length=8, n_rnn_layers=2),\n",
    "TCNModel(input_chunk_length=32, output_chunk_length=8,n_epochs = 120, random_state = 0),\n",
    "NBEATSModel(input_chunk_length=32, output_chunk_length=8,n_epochs = 120, random_state = 0),\n",
    "TransformerModel(input_chunk_length=32, output_chunk_length=8,n_epochs = 120, random_state = 0)]\n",
    "len(model_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa1d47a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running for Model : LinearRegression(n_jobs=-1)\n",
      "Running of details : ['SO_QTY_20210315', '2021-03-22', '2021-06-14']\n",
      "SO_QTY_20210315 2021-03-22 2021-06-14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashwini.kumar\\source\\repos\\anaconda3\\envs\\practice\\lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py:610: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  _check_optimize_result(\"lbfgs\", opt_res)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmark Loss is : 73494.5\n",
      "Model with covariates Predict Loss is : 23347.539882938083\n",
      "Running of details : ['SO_QTY_20210607', '2021-06-14', '2021-09-07']\n",
      "SO_QTY_20210607 2021-06-14 2021-09-07\n",
      "Benchmark Loss is : 41794.666666666664\n",
      "Model with covariates Predict Loss is : 26737.68865174483\n",
      "Running of details : ['SO_QTY_20210830', '2021-09-07', '2021-11-29']\n",
      "SO_QTY_20210830 2021-09-07 2021-11-29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashwini.kumar\\source\\repos\\anaconda3\\envs\\practice\\lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py:610: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  _check_optimize_result(\"lbfgs\", opt_res)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmark Loss is : 58756.5\n",
      "Model with covariates Predict Loss is : 41696.59676834417\n",
      "Running of details : ['SO_QTY_20211122', '2021-11-29', '2022-02-07']\n",
      "SO_QTY_20211122 2021-11-29 2022-02-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashwini.kumar\\source\\repos\\anaconda3\\envs\\practice\\lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py:610: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  _check_optimize_result(\"lbfgs\", opt_res)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmark Loss is : 48203.2\n",
      "Model with covariates Predict Loss is : 29069.329514491885\n",
      "running for Model : RandomForest(lags={'target': [-24, -23, -22, -21, -20, -19, -18, -17, -16, -15, -14, -13, -12, -11, -10, -9, -8, -7, -6, -5, -4, -3, -2, -1], 'past': [-6, -5, -4, -3, -2, -1]}, n_estimators=100, max_depth=None)\n",
      "Running of details : ['SO_QTY_20210315', '2021-03-22', '2021-06-14']\n",
      "SO_QTY_20210315 2021-03-22 2021-06-14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashwini.kumar\\source\\repos\\anaconda3\\envs\\practice\\lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py:610: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  _check_optimize_result(\"lbfgs\", opt_res)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmark Loss is : 73494.5\n",
      "Model with covariates Predict Loss is : 37265.96738723134\n",
      "Running of details : ['SO_QTY_20210607', '2021-06-14', '2021-09-07']\n",
      "SO_QTY_20210607 2021-06-14 2021-09-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashwini.kumar\\source\\repos\\anaconda3\\envs\\practice\\lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py:610: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  _check_optimize_result(\"lbfgs\", opt_res)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmark Loss is : 41794.666666666664\n",
      "Model with covariates Predict Loss is : 54403.47524590282\n",
      "Running of details : ['SO_QTY_20210830', '2021-09-07', '2021-11-29']\n",
      "SO_QTY_20210830 2021-09-07 2021-11-29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashwini.kumar\\source\\repos\\anaconda3\\envs\\practice\\lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py:610: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  _check_optimize_result(\"lbfgs\", opt_res)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmark Loss is : 58756.5\n",
      "Model with covariates Predict Loss is : 97509.16754769422\n",
      "Running of details : ['SO_QTY_20211122', '2021-11-29', '2022-02-07']\n",
      "SO_QTY_20211122 2021-11-29 2022-02-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashwini.kumar\\source\\repos\\anaconda3\\envs\\practice\\lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py:610: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  _check_optimize_result(\"lbfgs\", opt_res)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmark Loss is : 48203.2\n",
      "Model with covariates Predict Loss is : 88316.19090197576\n",
      "running for Model : <darts.models.forecasting.block_rnn_model.BlockRNNModel object at 0x0000016F674EBBC8>\n",
      "Running of details : ['SO_QTY_20210315', '2021-03-22', '2021-06-14']\n",
      "SO_QTY_20210315 2021-03-22 2021-06-14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashwini.kumar\\source\\repos\\anaconda3\\envs\\practice\\lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py:610: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  _check_optimize_result(\"lbfgs\", opt_res)\n",
      "[2022-04-18 17:14:52,224] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 181 samples.\n",
      "[2022-04-18 17:14:52,224] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 181 samples.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "[2022-04-18 17:14:52,242] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = BlockRNNModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-04-18 17:14:52,242] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = BlockRNNModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d05fc330f6343f580b5839f5fc6534a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e26f41f73164eac886ab688421dc4ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 6it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmark Loss is : 73494.5\n",
      "Model with covariates Predict Loss is : 183654.1730061183\n",
      "Running of details : ['SO_QTY_20210607', '2021-06-14', '2021-09-07']\n",
      "SO_QTY_20210607 2021-06-14 2021-09-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashwini.kumar\\source\\repos\\anaconda3\\envs\\practice\\lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py:610: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  _check_optimize_result(\"lbfgs\", opt_res)\n",
      "[2022-04-18 17:16:06,843] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 193 samples.\n",
      "[2022-04-18 17:16:06,843] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 193 samples.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "[2022-04-18 17:16:06,861] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = BlockRNNModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-04-18 17:16:06,861] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = BlockRNNModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a5c1d9975e74ce1849438e5ecd7abd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "219a68c703414845a76eb1162a328f75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 7it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmark Loss is : 41794.666666666664\n",
      "Model with covariates Predict Loss is : 159951.15778100406\n",
      "Running of details : ['SO_QTY_20210830', '2021-09-07', '2021-11-29']\n",
      "SO_QTY_20210830 2021-09-07 2021-11-29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashwini.kumar\\source\\repos\\anaconda3\\envs\\practice\\lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py:610: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  _check_optimize_result(\"lbfgs\", opt_res)\n",
      "[2022-04-18 17:17:24,413] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 205 samples.\n",
      "[2022-04-18 17:17:24,413] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 205 samples.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "[2022-04-18 17:17:24,433] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = BlockRNNModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-04-18 17:17:24,433] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = BlockRNNModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac816c8b4c354959983594b8ba4d725a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf9cc111f3e74180865149fe8fc46cd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 7it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmark Loss is : 58756.5\n",
      "Model with covariates Predict Loss is : 249210.31663828623\n",
      "Running of details : ['SO_QTY_20211122', '2021-11-29', '2022-02-07']\n",
      "SO_QTY_20211122 2021-11-29 2022-02-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashwini.kumar\\source\\repos\\anaconda3\\envs\\practice\\lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py:610: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  _check_optimize_result(\"lbfgs\", opt_res)\n",
      "[2022-04-18 17:18:45,211] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 217 samples.\n",
      "[2022-04-18 17:18:45,211] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 217 samples.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "[2022-04-18 17:18:45,230] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = BlockRNNModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-04-18 17:18:45,230] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = BlockRNNModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "465c87c9f34c41b1801e868dea342500",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "983b24afc05942f5864cdf1b7f5c15fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 7it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmark Loss is : 48203.2\n",
      "Model with covariates Predict Loss is : 218516.54486339694\n",
      "running for Model : <darts.models.forecasting.tcn_model.TCNModel object at 0x0000016F67401548>\n",
      "Running of details : ['SO_QTY_20210315', '2021-03-22', '2021-06-14']\n",
      "SO_QTY_20210315 2021-03-22 2021-06-14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-04-18 17:19:59,461] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 181 samples.\n",
      "[2022-04-18 17:19:59,461] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 181 samples.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "[2022-04-18 17:19:59,480] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TCNModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-04-18 17:19:59,480] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TCNModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ae4024a852748ac91a7efbde5dbfe8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5ef95de42734617b2856c1581c79886",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 6it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmark Loss is : 73494.5\n",
      "Model with covariates Predict Loss is : 42427.512402844026\n",
      "Running of details : ['SO_QTY_20210607', '2021-06-14', '2021-09-07']\n",
      "SO_QTY_20210607 2021-06-14 2021-09-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashwini.kumar\\source\\repos\\anaconda3\\envs\\practice\\lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py:610: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  _check_optimize_result(\"lbfgs\", opt_res)\n",
      "[2022-04-18 17:21:20,473] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 193 samples.\n",
      "[2022-04-18 17:21:20,473] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 193 samples.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "[2022-04-18 17:21:20,490] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TCNModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-04-18 17:21:20,490] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TCNModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6017220391e34104acb5355222cb354a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80d2eadd80a74d979b94d6ee62cd57fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 7it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmark Loss is : 41794.666666666664\n",
      "Model with covariates Predict Loss is : 60461.039733316145\n",
      "Running of details : ['SO_QTY_20210830', '2021-09-07', '2021-11-29']\n",
      "SO_QTY_20210830 2021-09-07 2021-11-29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashwini.kumar\\source\\repos\\anaconda3\\envs\\practice\\lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py:610: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  _check_optimize_result(\"lbfgs\", opt_res)\n",
      "[2022-04-18 17:22:33,891] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 205 samples.\n",
      "[2022-04-18 17:22:33,891] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 205 samples.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "[2022-04-18 17:22:33,908] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TCNModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-04-18 17:22:33,908] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TCNModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6b539a7b8a34ffb9d0b2d25d4305e9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f5c52a5036c488f92a9a05e51915e2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 7it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmark Loss is : 58756.5\n",
      "Model with covariates Predict Loss is : 133425.22037943374\n",
      "Running of details : ['SO_QTY_20211122', '2021-11-29', '2022-02-07']\n",
      "SO_QTY_20211122 2021-11-29 2022-02-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashwini.kumar\\source\\repos\\anaconda3\\envs\\practice\\lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py:610: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  _check_optimize_result(\"lbfgs\", opt_res)\n",
      "[2022-04-18 17:23:58,401] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 217 samples.\n",
      "[2022-04-18 17:23:58,401] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 217 samples.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "[2022-04-18 17:23:58,421] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TCNModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-04-18 17:23:58,421] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TCNModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d52ec9db1a8f425d8ff8776db1366b6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f213134da0b4828bb2cec5a6211ecf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 7it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmark Loss is : 48203.2\n",
      "Model with covariates Predict Loss is : 83487.63412953634\n",
      "running for Model : <darts.models.forecasting.nbeats.NBEATSModel object at 0x0000016F1B532848>\n",
      "Running of details : ['SO_QTY_20210315', '2021-03-22', '2021-06-14']\n",
      "SO_QTY_20210315 2021-03-22 2021-06-14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashwini.kumar\\source\\repos\\anaconda3\\envs\\practice\\lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py:610: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  _check_optimize_result(\"lbfgs\", opt_res)\n",
      "[2022-04-18 17:27:07,504] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 181 samples.\n",
      "[2022-04-18 17:27:07,504] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 181 samples.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "[2022-04-18 17:27:07,515] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = NBEATSModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-04-18 17:27:07,515] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = NBEATSModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c841dc89594c4e3fa78713115ff199ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72c99c52af9f45e499a09e06532426c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 6it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmark Loss is : 73494.5\n",
      "Model with covariates Predict Loss is : 26818.54827496041\n",
      "Running of details : ['SO_QTY_20210607', '2021-06-14', '2021-09-07']\n",
      "SO_QTY_20210607 2021-06-14 2021-09-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashwini.kumar\\source\\repos\\anaconda3\\envs\\practice\\lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py:610: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  _check_optimize_result(\"lbfgs\", opt_res)\n",
      "[2022-04-18 17:33:03,899] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 193 samples.\n",
      "[2022-04-18 17:33:03,899] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 193 samples.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "[2022-04-18 17:33:03,919] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = NBEATSModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-04-18 17:33:03,919] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = NBEATSModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdf14b73c37244fc8b6c289c1031deda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb2e906e8b614a3c9161db6afbce6a89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 7it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmark Loss is : 41794.666666666664\n",
      "Model with covariates Predict Loss is : 29018.66242603902\n",
      "Running of details : ['SO_QTY_20210830', '2021-09-07', '2021-11-29']\n",
      "SO_QTY_20210830 2021-09-07 2021-11-29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashwini.kumar\\source\\repos\\anaconda3\\envs\\practice\\lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py:610: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  _check_optimize_result(\"lbfgs\", opt_res)\n",
      "[2022-04-18 17:39:57,153] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 205 samples.\n",
      "[2022-04-18 17:39:57,153] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 205 samples.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "[2022-04-18 17:39:57,165] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = NBEATSModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-04-18 17:39:57,165] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = NBEATSModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2884e29796c4f52935b7f7e5da51fdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53597b33dc0b4f74b0ad3733ca31cbb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 7it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmark Loss is : 58756.5\n",
      "Model with covariates Predict Loss is : 138060.41131049197\n",
      "Running of details : ['SO_QTY_20211122', '2021-11-29', '2022-02-07']\n",
      "SO_QTY_20211122 2021-11-29 2022-02-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashwini.kumar\\source\\repos\\anaconda3\\envs\\practice\\lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py:610: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  _check_optimize_result(\"lbfgs\", opt_res)\n",
      "[2022-04-18 17:46:38,437] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 217 samples.\n",
      "[2022-04-18 17:46:38,437] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 217 samples.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "[2022-04-18 17:46:38,454] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = NBEATSModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-04-18 17:46:38,454] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = NBEATSModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bbb696cc3ff4599b0b23c2d47a727db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23b7b624f4c848acaa22722d1182249c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 7it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmark Loss is : 48203.2\n",
      "Model with covariates Predict Loss is : 42380.915008125\n",
      "running for Model : <darts.models.forecasting.transformer_model.TransformerModel object at 0x0000016F674EBF08>\n",
      "Running of details : ['SO_QTY_20210315', '2021-03-22', '2021-06-14']\n",
      "SO_QTY_20210315 2021-03-22 2021-06-14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashwini.kumar\\source\\repos\\anaconda3\\envs\\practice\\lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py:610: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  _check_optimize_result(\"lbfgs\", opt_res)\n",
      "C:\\Users\\ashwini.kumar\\source\\repos\\anaconda3\\envs\\practice\\lib\\site-packages\\ipykernel_launcher.py:18: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "[2022-04-18 17:53:39,542] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 181 samples.\n",
      "[2022-04-18 17:53:39,542] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 181 samples.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "[2022-04-18 17:53:39,561] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TransformerModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-04-18 17:53:39,561] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TransformerModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0321794988b544cfa808b0eb0b7c8ed4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e47ddb21a6d45debb9d84aadd220395",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 6it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmark Loss is : 73494.5\n",
      "Model with covariates Predict Loss is : 180871.38366217128\n",
      "Running of details : ['SO_QTY_20210607', '2021-06-14', '2021-09-07']\n",
      "SO_QTY_20210607 2021-06-14 2021-09-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashwini.kumar\\source\\repos\\anaconda3\\envs\\practice\\lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py:610: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  _check_optimize_result(\"lbfgs\", opt_res)\n",
      "[2022-04-18 17:57:37,799] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 193 samples.\n",
      "[2022-04-18 17:57:37,799] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 193 samples.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "[2022-04-18 17:57:37,820] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TransformerModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-04-18 17:57:37,820] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TransformerModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "095d3a9841044e8591619ed1fe2763bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "count = 0\n",
    "for j in range(len(model_list_covar)):\n",
    "    print (\"running for Model :\",model_list_covar[j])\n",
    "    for i in range(4):\n",
    "\n",
    "        periods = period_list[i]\n",
    "\n",
    "        print (\"Running of details :\",periods)\n",
    "        print (periods[0],periods[1],periods[2])\n",
    "\n",
    "        # Split Data - Required column \n",
    "        train_data,val_data,benchmark_data,timeseries_input,regression_train_data,regression_val_data_split = call_timeseries('Sum_All_Variables','Sum_All_Variables_Benchmark',periods[0],\n",
    "                                                    periods[1],periods[2])\n",
    "\n",
    "\n",
    "\n",
    "#         model_orig = model_list_orig[j]\n",
    "\n",
    "#         model_orig.fit(train_data)\n",
    "\n",
    "#         if i <=2:\n",
    "            \n",
    "#             pred1 = model_orig.predict(n=12 )\n",
    "#         else:\n",
    "#             pred1 = model_orig.predict(n=10 )\n",
    "#         # Calculate the loss off model\n",
    "#         loss_model_orig = mae(val_data , pred1)\n",
    "#         #Calculate the loss of bencmark model\n",
    "#         loss_benchmark_orig = mae(val_data, benchmark_data)\n",
    "#         # Calculate the loss off backtesting\n",
    "#         # loss_backtesting = mae(val_data,pred_backtest)\n",
    "#         # Print the loss function for benchmark data\n",
    "#         print (\"Benchmark Loss is :\", loss_benchmark_orig) \n",
    "#         # Print the loss function for benchmark data\n",
    "#         print (\"Model without covariates Predict Loss is :\", loss_model_orig)   \n",
    "        \n",
    "#         df_results.loc[count, 'Model_Name'] =model_list_covar[j]\n",
    "#         df_results.loc[count, 'period'] = periods[0]\n",
    "#         df_results.loc[count, 'loss_without_covar'] = loss_model_orig\n",
    "#         df_results.loc[count, 'loss_benchmark'] = loss_benchmark_orig\n",
    "\n",
    "        model = model_list_covar[j]\n",
    "\n",
    "        model.fit(train_data, \n",
    "                         past_covariates=regression_train_data)\n",
    "\n",
    "        if i <=2:\n",
    "            \n",
    "            pred = model.predict(n=12 )\n",
    "        else:\n",
    "            pred = model.predict(n=10 )\n",
    "\n",
    "        # Calculate the loss off model\n",
    "        loss_model_covariates = mae(val_data , pred)\n",
    "        #Calculate the loss of bencmark model\n",
    "        loss_benchmark = mae(val_data, benchmark_data)\n",
    "        # Calculate the loss off backtesting\n",
    "        # loss_backtesting = mae(val_data,pred_backtest)\n",
    "        # Print the loss function for benchmark data\n",
    "        print (\"Benchmark Loss is :\", loss_benchmark) \n",
    "        # Print the loss function for benchmark data\n",
    "        print (\"Model with covariates Predict Loss is :\", loss_model_covariates)   \n",
    "        df_results.loc[count, 'loss_with_covar'] = loss_model_covariates\n",
    "        df_results.loc[count, 'train_series'] = 'Sum_All_Variables'\n",
    "        df_results.loc[count, 'benchmark_series'] = 'Sum_All_Variables_Benchmark'\n",
    "        count = count + 1\n",
    "\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01dae66",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.to_csv(\"All_models_sum_variables_covariates.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b815a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
